# Gateway Implementation Status

## âœ… Completed

### 1. Lambda Deployer (FULLY IMPLEMENTED)
**File**: `logic/platforms/aws-lambda/deploy/deployer.py`

**Features**:
- âœ… Full boto3 implementation
- âœ… Create/update/delete Lambda functions
- âœ… Auto-create IAM execution role if needed
- âœ… Package handler code + dependencies
- âœ… Include core runtime library in package
- âœ… Smart pip install (skip unnecessary files)
- âœ… Wait for function to be active
- âœ… List functions with prefix filter
- âœ… Get function ARN
- âœ… Configuration updates (memory, timeout, etc.)

**Usage**:
```python
from logic.platforms.aws_lambda.deploy import LambdaDeployer

deployer = LambdaDeployer(region='us-east-1', profile='my-profile')
deployer.deploy_function(
    function_name='carto-at-quadbin-polyfill',
    handler_file=Path('handler.py'),
    requirements_file=Path('requirements.txt'),
    memory_size=512,
    timeout=60
)
```

### 2. Core Structure
- âœ… All logic in `core/gateway/`
- âœ… Private functions in `analytics-toolbox/gateway/`
- âœ… Example public function (quadbin_polyfill)
- âœ… Example private function (geocode)
- âœ… Complete test structure

### 3. Package Builder
- âœ… Generates distribution packages
- âœ… CDK infrastructure (skeleton)
- âœ… SQL templates
- âœ… Installer script (skeleton)

## âš ï¸ Needs Completion

### 1. Configuration Loader
**File**: `logic/common/engine/config_loader.py` (TO CREATE)

**What's needed**:
```python
class ConfigLoader:
    """Load and merge configuration from YAML files"""

    def load_config(self, config_path: Path) -> Dict[str, Any]:
        """Load config from YAML"""
        with open(config_path) as f:
            return yaml.safe_load(f)

    def merge_configs(self, base: Dict, override: Dict) -> Dict:
        """Deep merge configurations"""
        # Implement recursive merge
        pass

    def get_aws_config(self, config: Dict) -> Dict:
        """Extract AWS configuration"""
        return config.get('aws', {})

    def get_lambda_config(self, config: Dict) -> Dict:
        """Extract Lambda configuration"""
        return config.get('lambda', {})
```

**Integration with CLI**:
```python
# In cli.py
@cli.command()
@click.option('--config', type=click.Path(exists=True, path_type=Path))
def deploy_all(config: Path):
    # Load configuration
    config_loader = ConfigLoader()
    cfg = config_loader.load_config(config)

    # Use configuration
    aws_config = config_loader.get_aws_config(cfg)
    lambda_config = config_loader.get_lambda_config(cfg)

    deployer = LambdaDeployer(
        region=aws_config.get('region', 'us-east-1'),
        profile=aws_config.get('profile')
    )
```

### 2. CLI with Filters (PATTERN FROM clouds/)
**File**: `logic/clouds/redshift/cli.py` (UPDATE)

**Add filter options** (like `clouds/redshift/modules/Makefile`):
```python
@cli.command()
@click.option('--cloud', default='redshift', help='Cloud to deploy (redshift, bigquery, etc.)')
@click.option('--modules', help='Comma-separated categories to deploy (quadbin,h3,tiler)')
@click.option('--functions', help='Comma-separated function names to deploy')
@click.option('--diff', is_flag=True, help='Only deploy modified functions (based on git)')
def deploy_all(cloud, modules, functions, diff):
    """Deploy with filters"""

    loader = CatalogLoader(functions_root)
    loader.load_catalog()

    # Filter by cloud
    to_deploy = loader.get_functions_by_cloud(CloudType(cloud))

    # Filter by modules (categories)
    if modules:
        categories = [m.strip() for m in modules.split(',')]
        to_deploy = [f for f in to_deploy if f.category in categories]

    # Filter by function names
    if functions:
        func_names = [f.strip() for f in functions.split(',')]
        to_deploy = [f for f in to_deploy if f.name in func_names]

    # Filter by git diff
    if diff:
        modified_files = get_modified_files()  # Get from git
        to_deploy = [f for f in to_deploy if is_function_modified(f, modified_files)]

    # Deploy filtered list
    for func in to_deploy:
        deploy_function(func)
```

**Update Makefile** to match clouds pattern:
```makefile
# In core/gateway/Makefile

deploy: ## Deploy functions with filters
	@echo "Deploying functions..."
	cd logic/clouds/redshift && $(PYTHON) cli.py deploy-all \
		--cloud=$(cloud) \
		--modules=$(modules) \
		--functions=$(functions) \
		--diff=$(diff)

# Examples:
# make deploy cloud=redshift modules=quadbin,h3
# make deploy cloud=redshift functions=quadbin_polyfill,quadbin_boundary
# make deploy cloud=redshift diff=1  # Only modified
```

### 3. Custom Resource Handler
**File**: `cdk/src/register-external-function/index.py` (IN PACKAGE)

**What's needed**:
```python
import boto3
import json
from jinja2 import Template

redshift_data = boto3.client('redshift-data')

def handler(event, context):
    request_type = event['RequestType']
    properties = event['ResourceProperties']

    cluster_id = properties['ClusterIdentifier']
    database = properties['Database']
    schema = properties['Schema']
    lambda_arns = properties['LambdaArns']  # Dict: {function_name: arn}
    iam_role_arn = properties['IamRoleArn']

    if request_type == 'Create':
        return create_functions(cluster_id, database, schema, lambda_arns, iam_role_arn)
    elif request_type == 'Update':
        return update_functions(cluster_id, database, schema, lambda_arns, iam_role_arn)
    elif request_type == 'Delete':
        return delete_functions(cluster_id, database, schema, lambda_arns)

def create_functions(cluster_id, database, schema, lambda_arns, iam_role_arn):
    """Create external functions in Redshift"""

    # Load SQL template
    with open('external_functions.sql.j2') as f:
        template = Template(f.read())

    # Render SQL with Lambda ARNs
    sql = template.render(
        schema=schema,
        lambda_arns=lambda_arns,
        iam_role_arn=iam_role_arn
    )

    # Execute SQL using Redshift Data API
    response = redshift_data.execute_statement(
        ClusterIdentifier=cluster_id,
        Database=database,
        Sql=sql
    )

    # Wait for completion
    statement_id = response['Id']
    wait_for_statement(statement_id)

    return {
        'PhysicalResourceId': f"{cluster_id}-{database}-{schema}-external-functions",
        'Data': {'Status': 'Created', 'StatementId': statement_id}
    }

def wait_for_statement(statement_id, max_wait=300):
    """Wait for Redshift Data API statement to complete"""
    import time
    start = time.time()

    while time.time() - start < max_wait:
        response = redshift_data.describe_statement(Id=statement_id)
        status = response['Status']

        if status == 'FINISHED':
            return True
        elif status in ['FAILED', 'ABORTED']:
            error = response.get('Error', 'Unknown error')
            raise Exception(f"Statement failed: {error}")

        time.sleep(5)

    raise Exception(f"Statement timeout after {max_wait}s")
```

### 4. Installer Validation
**File**: `scripts/install.py` (IN PACKAGE)

**Pre-flight checks needed**:
```python
import click
import subprocess
import sys
from pathlib import Path

def validate_environment():
    """Pre-flight validation checks"""

    checks = []

    # Check Python version
    if sys.version_info < (3, 11):
        checks.append(("Python 3.11+", False, f"Found {sys.version}"))
    else:
        checks.append(("Python 3.11+", True, f"{sys.version}"))

    # Check AWS CLI
    try:
        result = subprocess.run(['aws', '--version'], capture_output=True, text=True)
        checks.append(("AWS CLI", True, result.stdout.strip()))
    except FileNotFoundError:
        checks.append(("AWS CLI", False, "Not found"))

    # Check AWS credentials
    try:
        result = subprocess.run(['aws', 'sts', 'get-caller-identity'],
                                capture_output=True, text=True)
        if result.returncode == 0:
            checks.append(("AWS Credentials", True, "Configured"))
        else:
            checks.append(("AWS Credentials", False, "Not configured"))
    except Exception as e:
        checks.append(("AWS Credentials", False, str(e)))

    # Check CDK
    try:
        result = subprocess.run(['cdk', '--version'], capture_output=True, text=True)
        checks.append(("AWS CDK", True, result.stdout.strip()))
    except FileNotFoundError:
        checks.append(("AWS CDK", False, "Not installed. Run: npm install -g aws-cdk"))

    # Check Node.js
    try:
        result = subprocess.run(['node', '--version'], capture_output=True, text=True)
        checks.append(("Node.js", True, result.stdout.strip()))
    except FileNotFoundError:
        checks.append(("Node.js", False, "Not found"))

    # Check npm
    try:
        result = subprocess.run(['npm', '--version'], capture_output=True, text=True)
        checks.append(("npm", True, result.stdout.strip()))
    except FileNotFoundError:
        checks.append(("npm", False, "Not found"))

    # Print results
    click.echo("\n" + "="*60)
    click.echo("Pre-flight Environment Check")
    click.echo("="*60)

    all_passed = True
    for check_name, passed, message in checks:
        status = click.style("âœ“", fg='green') if passed else click.style("âœ—", fg='red')
        click.echo(f"{status} {check_name}: {message}")
        if not passed:
            all_passed = False

    click.echo("="*60)

    return all_passed

def check_aws_permissions():
    """Check if user has required AWS permissions"""
    import boto3

    required_permissions = [
        ('lambda', 'list_functions'),
        ('iam', 'list_roles'),
        ('cloudformation', 'list_stacks'),
        ('redshift', 'describe_clusters')
    ]

    click.echo("\nChecking AWS permissions...")
    all_passed = True

    for service, operation in required_permissions:
        try:
            client = boto3.client(service)
            getattr(client, operation)()
            click.echo(f"  âœ“ {service}:{operation}")
        except Exception as e:
            click.echo(f"  âœ— {service}:{operation} - {e}")
            all_passed = False

    return all_passed

@click.command()
@click.option('--skip-validation', is_flag=True, help='Skip pre-flight checks')
def install(skip_validation):
    """Install CARTO Analytics Toolbox"""

    if not skip_validation:
        if not validate_environment():
            click.echo("\nâŒ Environment validation failed. Fix issues and try again.")
            sys.exit(1)

        if not check_aws_permissions():
            click.echo("\nâŒ AWS permission check failed. Ensure you have required permissions.")
            sys.exit(1)

    # Continue with installation...
```

## ðŸ“‹ Implementation Priority

### Phase 1: Core Deployment (DONE âœ…)
1. âœ… Lambda Deployer with boto3
2. âœ… Complete packaging logic

### Phase 2: Configuration & Filters (NEXT)
1. âš ï¸ Config loader from YAML
2. âš ï¸ CLI filters (cloud, modules, functions, diff)
3. âš ï¸ Makefile updates to match clouds/ pattern

### Phase 3: Distribution Package (AFTER)
4. âš ï¸ Custom resource handler
5. âš ï¸ Installer with validation
6. âš ï¸ Complete CDK TypeScript stack

## ðŸŽ¯ How to Use Current Implementation

### Deploy Single Function
```bash
cd core/gateway
python logic/clouds/redshift/cli.py deploy-lambda quadbin_polyfill \
  --aws-profile my-profile \
  --region us-east-1
```

### Create Package
```bash
cd core/gateway
python logic/clouds/redshift/cli.py create-package ./dist \
  --version 1.0.0
```

### Using Lambda Deployer Directly
```python
from pathlib import Path
from logic.platforms.aws_lambda.deploy import LambdaDeployer

# Initialize deployer
deployer = LambdaDeployer(region='us-east-1', profile='default')

# Deploy a function
deployer.deploy_function(
    function_name='carto-at-test',
    handler_file=Path('functions/quadbin/quadbin_polyfill/code/lambda/python/handler.py'),
    requirements_file=Path('functions/quadbin/quadbin_polyfill/code/lambda/python/requirements.txt'),
    memory_size=512,
    timeout=60,
    description="Test deployment"
)

# Get function ARN
arn = deployer.get_function_arn('carto-at-test')
print(f"Function ARN: {arn}")
```

## ðŸ“ Notes

- Lambda deployer is **production-ready** with full boto3 implementation
- Auto-creates IAM roles if they don't exist
- Includes core runtime library in every package
- Smart dependency installation (skips tests, docs, etc.)
- Configuration loader and filters need implementation
- Custom resource and installer are documented but need coding

## ðŸ”— Related Files

- Lambda Deployer: `logic/platforms/aws-lambda/deploy/deployer.py` âœ…
- CLI: `logic/clouds/redshift/cli.py` âš ï¸ (needs filters)
- Config Loader: `logic/common/engine/config_loader.py` âŒ (to create)
- Custom Resource: In distribution package âš ï¸ (skeleton exists)
- Installer: In distribution package âš ï¸ (skeleton exists)
